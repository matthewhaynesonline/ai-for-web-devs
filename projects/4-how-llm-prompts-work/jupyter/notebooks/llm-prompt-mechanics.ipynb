{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c01b8596-d55b-4dd6-a25a-46c129877424",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.3.1+cpu)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccf24d20-6987-4dd2-8a3e-49f3008ec57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Using cached huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.11/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->transformers) (2024.6.2)\n",
      "Using cached transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "Using cached huggingface_hub-0.23.4-py3-none-any.whl (402 kB)\n",
      "Using cached regex-2024.5.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "Using cached safetensors-0.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.4 regex-2024.5.15 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.42.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc50a7df-d85b-4e59-bb86-8a99a95c7081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2771e0f0-f929-4e0c-925f-5fd0d014f7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/docs/transformers/en/quicktour\n",
    "hf_result = pipeline(\"sentiment-analysis\")(\"we love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "612ebcc5-92f4-4f8c-988b-db243e693e03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9998704195022583}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5cd8000-05c8-456a-a08a-15bc81038107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70bdbebdd1454b7c88df89e4431bab7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/560 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9192ef86afd243f1a5cd31634394f2ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/4.40G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21723732ef2f405cb70d504fd59c78a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/129 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09b77e1599354f4589893313d934e169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/776 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704ff0593be448a19f424a2b93e0d49c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9de07e5289aa420fb83b72c9e7e4665a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b07fe359ac04f11a95ba26cd2f09e30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODELS = [\n",
    "    \"TinyLlama/TinyLlama_v1.1\",\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "]\n",
    "\n",
    "TENSOR_TYPE = \"pt\"\n",
    "\n",
    "model_id = MODELS[0]\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "max_new_tokens = 512\n",
    "temperature = 0.7\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a pirate chatbot who always responds in pirate speak!\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"Hi.\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "922fd117-496d-4452-a3a5-a4f6ca4f97d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/docs/transformers/en/conversations#what-happens-inside-the-pipeline\n",
    "formatted_chat = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "\n",
    "tokenized_inputs = tokenizer(\n",
    "    formatted_chat, return_tensors=TENSOR_TYPE, add_special_tokens=False\n",
    ")\n",
    "inputs = {\n",
    "    key: tensor.to(model.device)\n",
    "    for key, tensor in tokenized_inputs.items()\n",
    "}\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs, max_new_tokens=max_new_tokens, temperature=temperature, do_sample=True\n",
    ")\n",
    "decoded_output = tokenizer.decode(\n",
    "    outputs[0][inputs[\"input_ids\"].size(1) :], skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96dcc1b0-4969-45f3-adb7-ddcb4392ef73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\nYou are a pirate chatbot who always responds in pirate speak!\\n<</SYS>>\\n\\nHi. [/INST]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "164567d4-7b58-4de2-9382-d251e687c215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13,  3492,\n",
       "           526,   263, 21625,   403, 13563,  7451,  1058,  2337, 10049, 29879,\n",
       "           297, 21625,   403,  7726, 29991,    13, 29966,   829, 14816, 29903,\n",
       "          6778,    13,    13, 18567, 29889,   518, 29914, 25580, 29962]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be877bca-90ab-43e9-8fde-82711a0a8b94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13,  3492,\n",
       "            526,   263, 21625,   403, 13563,  7451,  1058,  2337, 10049, 29879,\n",
       "            297, 21625,   403,  7726, 29991,    13, 29966,   829, 14816, 29903,\n",
       "           6778,    13,    13, 18567, 29889,   518, 29914, 25580, 29962]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dec2d2ab-9e05-4aa7-9dff-5d85f558b672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   518, 25580, 29962,  3532, 14816, 29903,  6778,    13,  3492,\n",
       "           526,   263, 21625,   403, 13563,  7451,  1058,  2337, 10049, 29879,\n",
       "           297, 21625,   403,  7726, 29991,    13, 29966,   829, 14816, 29903,\n",
       "          6778,    13,    13, 18567, 29889,   518, 29914, 25580, 29962,    13,\n",
       "         29966,   829, 14816, 29903,  6778,    13,    13, 18567, 29889,   518,\n",
       "         29914, 25580, 29962,    13, 29966,   829, 14816, 29903,  6778,    13,\n",
       "            13, 18567, 29889,   518, 29914, 25580, 29962,    13, 29966,   829,\n",
       "         14816, 29903,  6778,    13,    13, 18567, 29889,   518, 29914, 25580,\n",
       "         29962,    13, 29966,   829, 14816, 29903,  6778,    13,    13, 18567,\n",
       "         29889,   518, 29914, 25580, 29962,    13, 29966,   829, 14816, 29903,\n",
       "          6778,    13,    13, 18567, 29889,   518, 29914, 25580, 29962,    13,\n",
       "         29966,   829, 14816, 29903,  6778,    13,    13, 18567, 29889,   518,\n",
       "         29914, 25580, 29962,    13, 29966,   829, 14816, 29903,  6778,    13,\n",
       "            13, 18567, 29889,   518, 29914, 25580, 29962,    13, 29966,   829,\n",
       "         14816, 29903,  6778,    13,    13, 18567, 29889,   518, 29914, 25580,\n",
       "         29962,    13, 29966,   829, 14816, 29903,  6778,    13,    13, 18567,\n",
       "         29889,   518, 29914, 25580, 29962,    13, 29966,   829, 14816, 29903,\n",
       "          6778,    13,    13, 18567, 29889,   518, 29914, 25580, 29962,    13,\n",
       "         29966,   829, 14816, 29903,  6778,    13,    13, 18567, 29889,   518,\n",
       "         29914, 25580, 29962,    13, 29966,   829, 14816, 29903,  6778,    13,\n",
       "            13, 18567, 29889,   518, 29914, 25580, 29962,    13, 29966,   829,\n",
       "         14816, 29903,  6778,    13,    13, 18567, 29889,   518, 29914, 25580,\n",
       "         29962,    13, 29966,   829, 14816, 29903,  6778,    13,    13, 18567,\n",
       "         29889,   518, 29914, 25580, 29962,    13, 29966,   829, 14816, 29903,\n",
       "          6778,    13,    13, 18567, 29889,   518, 29914, 25580, 29962,    13,\n",
       "         29966,   829, 14816, 29903,  6778,    13,    13, 18567, 29889,   518,\n",
       "         29914, 25580, 29962,    13, 29966,   829, 14816, 29903,  6778,    13,\n",
       "            13, 18567, 29889,   518, 29914, 25580, 29962,    13, 29966,   829,\n",
       "         14816, 29903,  6778,    13,    13, 18567, 29889,   518, 29914, 25580,\n",
       "         29962,    13, 29966,   829, 14816, 29903,  6778,    13,    13, 18567,\n",
       "         29889,   518, 29914, 25580, 29962,    13, 29966,   829, 14816, 29903,\n",
       "          6778,    13,    13, 18567, 29889,   518, 29914, 25580, 29962,    13,\n",
       "         29966,   829, 14816, 29903,  6778,    13,    13, 18567, 29889,   518,\n",
       "         29914, 25580, 29962,    13, 29966,   829, 14816, 29903,  6778,    13,\n",
       "            13, 18567, 29889,   518, 29914, 25580, 29962,    13, 29966,   829,\n",
       "         14816, 29903,  6778,    13,    13, 18567, 29889,   518, 29914, 25580,\n",
       "         29962,    13, 29966,   829, 14816, 29903,  6778,    13,    13, 18567,\n",
       "         29889,   518, 29914, 25580, 29962,    13, 29966,   829, 14816, 29903,\n",
       "          6778,    13,    13, 18567, 29889,   518, 29914, 25580, 29962,    13,\n",
       "         29966,   829, 14816, 29903,  6778,    13,    13, 18567, 29889,   518,\n",
       "         29914, 25580, 29962,    13, 29966,   829, 14816, 29903,  6778,    13,\n",
       "            13, 18567, 29889,   518, 29914, 25580, 29962,    13, 29966,   829,\n",
       "         14816, 29903,  6778,    13,    13, 18567, 29889,   518, 29914, 25580,\n",
       "         29962,    13, 29966,   829, 14816, 29903,  6778,    13,    13, 18567,\n",
       "         29889,   518, 29914, 25580, 29962,    13, 29966,   829, 14816, 29903,\n",
       "          6778,    13,    13, 18567, 29889,   518, 29914, 25580, 29962,    13,\n",
       "         29966,   829, 14816, 29903,  6778,    13,    13, 18567, 29889,   518,\n",
       "         29914, 25580, 29962,    13, 29966,   829, 14816, 29903,  6778,    13,\n",
       "            13, 18567, 29889,   518, 29914, 25580, 29962,    13, 29966,   829,\n",
       "         14816, 29903,  6778,    13,    13, 18567, 29889,   518, 29914, 25580,\n",
       "         29962,    13, 29966,   829, 14816, 29903,  6778,    13,    13, 18567,\n",
       "         29889,   518, 29914, 25580, 29962,    13, 29966,   829, 14816, 29903,\n",
       "          6778,    13,    13, 18567, 29889,   518, 29914, 25580, 29962,    13,\n",
       "         29966,   829, 14816, 29903,  6778,    13,    13, 18567, 29889,   518,\n",
       "         29914, 25580, 29962,    13, 29966,   829, 14816, 29903,  6778,    13,\n",
       "            13]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44dbda39-aa8e-46f2-add4-7d9747d028bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\nHi. [/INST]\\n<</SYS>>\\n\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3161a309-e7dc-4151-a661-a42762f95dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/blog/how-to-generate\n",
    "# Set seed to reproduce results.\n",
    "set_seed(42)\n",
    "\n",
    "input_text = \"I enjoy walking with my cute dog\"\n",
    "\n",
    "model_inputs = tokenizer(input_text, return_tensors=TENSOR_TYPE)\n",
    "\n",
    "sample_outputs = model.generate(\n",
    "    **model_inputs,\n",
    "    # max_new_tokens=max_new_tokens,\n",
    "    max_new_tokens=1,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    num_return_sequences=10,\n",
    "    temperature=temperature,\n",
    ")\n",
    "\n",
    "decoded_outputs = []\n",
    "\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "    decoded_outputs.append(\n",
    "        tokenizer.decode(sample_output, skip_special_tokens=False)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b70d8f1-0ad0-4783-a816-d84c3c0ab298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "041de171-022f-4d39-a5a0-1e9d392de04f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203,   274],\n",
       "        [    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203,  3460],\n",
       "        [    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203, 29889],\n",
       "        [    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203,  1641],\n",
       "        [    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203,   590],\n",
       "        [    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203, 29889],\n",
       "        [    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203,  2805],\n",
       "        [    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203, 29889],\n",
       "        [    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203,   590],\n",
       "        [    1,   306, 13389, 22049,   411,   590,   274,  1082, 11203, 22049]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1028bfb6-fd23-41c5-b052-3717250489cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<s> I enjoy walking with my cute dog c',\n",
       " '<s> I enjoy walking with my cute dogging',\n",
       " '<s> I enjoy walking with my cute dog.',\n",
       " '<s> I enjoy walking with my cute dog being',\n",
       " '<s> I enjoy walking with my cute dog my',\n",
       " '<s> I enjoy walking with my cute dog.',\n",
       " '<s> I enjoy walking with my cute dog getting',\n",
       " '<s> I enjoy walking with my cute dog.',\n",
       " '<s> I enjoy walking with my cute dog my',\n",
       " '<s> I enjoy walking with my cute dog walking']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3fb48c-1a7f-4cb5-a557-853469263479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
